{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d26384d3-c215-4f39-81ca-f6bd15753ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text('reset_all_data', 'false')\n",
    "\n",
    "# Change your schema here:\n",
    "catalog = \"mozuca\"\n",
    "schema = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae6f9c5b-a17c-4357-a033-6824776a4162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "\n",
    "if reset_all_data:\n",
    "  print(f'clearing up db {schema}')\n",
    "  spark.sql(f\"DROP DATABASE IF EXISTS `{schema}` CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f8db43d-c0d6-4646-b556-9587675766f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def use_and_create_db(catalog, schema, cloud_storage_path = None):\n",
    "  print(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"\"\"create database if not exists `{schema}` \"\"\")\n",
    "\n",
    "assert catalog not in ['hive_metastore', 'spark_catalog']\n",
    "#If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "if len(catalog) > 0:\n",
    "  current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "  if current_catalog != catalog:\n",
    "    catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if catalog not in catalogs:\n",
    "      spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "  use_and_create_db(catalog, schema)\n",
    "\n",
    "print(f\"using catalog.database `{catalog}`.`{schema}`\")\n",
    "spark.sql(f\"\"\"USE `{catalog}`.`{schema}`\"\"\")    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
